[
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_ft_mint()] [Cost Accounting] FT minting charges write_count: u2 and read_count: u2 - what are these four operations (likely supply update + recipient balance update, each requiring a read-before-write), and do constant costs of u1 per operation adequately prevent mint spam attacks? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_ft_transfer()] [Cost Symmetry] FT transfers charge the same costs as ft_mint (write_count: u2, read_count: u2, runtime: u1000) - is this appropriate, or should transfers be more expensive since they require checking two balances and updating both sender and receiver state? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_ft_burn()] [Cost Model] Burning FTs has identical costs to minting (write_count: u2, read_count: u2) - does this properly reflect that burning requires updating both supply and holder balance, or could the cost model be exploited by repeatedly minting and burning to exhaust state operations? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_nft_mint()] [Cost Scaling] NFT minting uses linear(n, u1000, u1000) runtime where n likely represents asset identifier size - is this appropriate for NFTs where the identifier might be a complex data structure, or could attackers mint NFTs with large identifiers while being undercharged? (High)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_nft_transfer()] [Cost Accounting] NFT transfers charge constant write_count: u1 and read_count: u1 despite using linear(n, u1000, u1000) runtime - if n represents asset identifier size, shouldn't read/write lengths also scale with n to reflect the actual storage I/O for large identifiers? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_nft_owner()] [Cost Model] Checking NFT ownership uses linear(n, u1000, u1000) runtime but constant read operations - if looking up ownership requires deserializing the asset identifier (size n), are constant read_count: u1 and read_length: u1 sufficient, or is this undercharged? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_nft_burn()] [Cost Consistency] NFT burning has identical costs to minting/transferring (linear runtime, constant read/write counts) - given that burning frees storage, should it be cheaper to incentivize cleanup, or does charging the same cost prevent burn spam that could cause state churn? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_add()] [Cost Model] Addition uses linear(n, u1000, u1000) where n likely represents bit-width - for native 128-bit uint addition which is O(1) in hardware, why charge linear cost, and could this overcharging cause legitimate calculations to hit cost limits unnecessarily? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_sub()] [Cost Consistency] Subtraction charges the same linear cost as addition - is this appropriate, or should subtraction be cheaper/more expensive depending on underflow checking overhead compared to overflow checking in addition? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_mul()] [Cost Model] Multiplication uses linear(n, u1000, u1000) but actual multiplication complexity for n-bit numbers is O(n log n) or O(n²) depending on algorithm - is linear cost appropriate, and could very large multiplications be undercharged relative to actual CPU cost? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_div()] [Cost Model] Division charges linear(n, u1000, u1000) but division algorithms are typically O(n²) for n-bit operands - is this significant undercharging, and could an attacker fill blocks with large-integer divisions to cause DoS while staying within cost limits? (High)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_mod()] [Cost Inconsistency] Modulo operation charges constant runtime: u1000 while div charges linear cost - since modulo is typically computed via division (a mod b = a - (a/b)*b), why is mod constant cost, and is this a critical undercharging that enables DoS? (High)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_pow()] [Cost Underestimation] Power operation charges only constant runtime: u1000 regardless of exponent size - exponentiation is typically O(log e) with fast exponentiation or O(e) naively, so can an attacker compute huge powers (2^127) at constant cost and cause computational DoS? (Critical)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_sqrti()] [Cost Model] Square root charges constant runtime: u1000 but sqrt algorithms like Newton's method have iteration counts dependent on bit-width - for 128-bit integers could sqrti actually take O(log n) or O(n) time while being charged O(1), enabling DoS? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_log2()] [Cost Model] Log2 charges constant runtime: u1000 which seems reasonable for bit-counting, but if the implementation uses iterative algorithms, could worst-case inputs cause the operation to take more than constant time while being charged constant cost? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_geq()] [Cost Model] Greater-or-equal comparison charges constant runtime: u1000 - for 128-bit integers is this adequate, or could comparing many large integers in a loop accumulate to more actual CPU cost than charged, enabling micro-DoS attacks? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_eq()] [Cost Model] Equality checking uses linear(n, u1000, u1000) suggesting it scales with data size - for complex types like lists or tuples, is linear cost sufficient, or could deep equality checks on nested structures be O(n²) or worse while charged O(n)? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_or()] [Cost Model] Logical OR uses linear(n, u1000, u1000) - if n represents number of operands being OR'ed, is this appropriate, or should short-circuit evaluation make some OR operations cheaper when an early operand is true? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_and()] [Cost Consistency] Logical AND has the same linear cost as OR - is this correct given that AND and OR should have similar complexity, or should AND be differently priced if it short-circuits differently? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_hash160()] [Cost Model] Hash160 uses linear(n, u1000, u1000) where n is input size - is a coefficient of u1000 per byte adequate for RIPEMD-160(SHA256(input)) double-hashing, or is this undercharged allowing hash flooding attacks? (High)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_sha256()] [Cost Model] SHA256 charges linear(n, u1000, u1000) - SHA256 processes 64-byte blocks with fixed per-block overhead, so is linear cost with u1000 per byte appropriate, or should there be a higher base cost b to reflect setup overhead? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_sha512()] [Cost Consistency] SHA512 charges the same linear cost as SHA256 despite processing 128-byte blocks and having different internal operations - should SHA512 be cheaper per byte due to larger block size, or more expensive due to 64-bit operations? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_sha512t256()] [Cost Model] SHA512/256 (truncated SHA512) charges the same as full SHA512 - is this appropriate, or should the truncated version be slightly cheaper since it discards half the output, or are the computational costs identical? (Low)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_keccak256()] [Cost Parity] Keccak256 charges the same linear cost as SHA256 - given that Keccak has different internal structure (sponge construction vs Merkle-Damgård), are the costs truly equivalent, or could one be more expensive to compute and thus exploitable? (Medium)",
  "[File: stackslib/src/chainstate/stacks/boot/costs.clar] [Function: cost_secp256k1recover()] [Cost Model] Secp256k1 recovery charges constant runtime: u1000 - elliptic curve operations are computationally expensive (milliseconds), so is u1000 cost units adequate to prevent attackers from filling blocks with signature recovery operations and causing DoS? (Critical)"
]